seed_everything: 42
float32_matmul_precision: medium
logging_level: DEBUG

trainer:
  strategy: 
    class_path: llm_training.overrides.DeepSpeedStrategy
    init_args:
      stage: 2
  precision: bf16-true
  logger:
    class_path: llm_training.overrides.wandb.WandbLogger
    init_args:
      name: phi-3-mini-128k-instruct_demo
      job_type: demo
      project: llm-training
      save_dir: logs
      save_code: true
  max_epochs: 1
  val_check_interval: null
  accumulate_grad_batches: 1
  gradient_clip_val: 1.0
  callbacks:
    - class_path: LearningRateMonitor
    - class_path: llm_training.overrides.ModelCheckpoint
      init_args:
        save_on_train_epoch_end: true
        save_top_k: -1
    - class_path: llm_training.overrides.ModelCheckpoint
      init_args:
        save_top_k: -1
        every_n_train_steps: 5000
    - class_path: llm_training.overrides.ModelCheckpoint
      init_args:
        save_top_k: 1
        every_n_train_steps: 1000

model:
  class_path: llm_training.models.hf_causal_lm.HFCausalLM
  init_args.config:
    hf_path: microsoft/Phi-3-mini-128k-instruct
    hf_model_kwargs:
      torch_dtype: bfloat16
      attn_implementation: flash_attention_2
    enable_gradient_checkpointing: true
    
    optim:
      optimizer_class: deepspeed.ops.adam.FusedAdam
      optimizer_kwargs:
        lr: 1e-4
      lr_scheduler_class: llm_training.lr_schedulers.CosineAnnealingWarmupLR
      lr_scheduler_kwargs:
        num_warmup_steps: 2000
        min_lr: 1e-5

data:
  class_path: llm_training.data.DummyDataModule
  init_args.config:
    batch_size: 1
    vocab_size: 32064
    max_length: 4096
    num_tokens: 50_000_000_000 # 50B
